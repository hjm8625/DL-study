# -*- coding: utf-8 -*-
"""LSTMstudy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mxautUQP7UxlSDm0eKcsslfvPK6UgOJ1

Simple LSTM with audio_mnist
"""

# use kaggle audio mnist (20/60)
# we need to make dataset class

from google.colab import drive
drive.mount('/content/drive')

# check the data -> we need to make labels
import os
import torch
import torchaudio
path = '/content/drive/MyDrive/data'
dataset = []
for i in range(1,31):
  number = f'{i:02d}' # ex) 01
  audiofolder = os.path.join(path, number)
  for audio in os.listdir(audiofolder): # search all files in audiofolder
    audiopath = os.path.join(audiofolder, audio) # access to real audio data
    # In file name, there are label, speaker, repeat number
    filename = os.path.basename(audiopath)
    audioname = filename.replace('wav','')
    label = filename.split('_')[0]
    speaker = filename.split('_')[1]
    repeat = filename.split('_')[2]
    waveform, samplerate = torchaudio.load(audiopath)
    label = torch.tensor(int(label))
    dataset.append({'waveform':waveform, 'sr':samplerate,'speaker':speaker,'label':label})
  if i % 5 == 0:
    print(f'Label {i}/30 complete')

# Watch data example
print(dataset[0])
print(len(dataset))

# I need to watch all the length of waveform is same
# To watch the length od tensor, use shape[1]. because tensor = 1*14073
A=[]
for i in range(len(dataset)):
  w = dataset[i]['waveform']
  A.append(w.shape[1])
print(f' Min : {min(A)}, Max: {max(A)}')

# So, I need to make same length -> 16KHz will be good. using nn.functional.pad
import torch.nn.functional as F
A=[]
for i in range(len(dataset)):
  w = dataset[i]['waveform']
  if w.shape[1] <20000:
    w =F.pad(w,(0,20000-w.shape[1])) # pad(x,(number, length to pad))
    dataset[i]['waveform'] = w[:,4000:20000]
  else:
    dataset[i]['waveform'] = w[:,4000:20000]
for i in range(len(dataset)):
  w = dataset[i]['waveform']
  A.append(w.shape[1])
print(f' Min : {min(A)}, Max: {max(A)}')

# make DATASET class to pytorch
from torch.utils.data import Dataset, DataLoader

# make init,len,getitem
class audiodataset(Dataset):
  def __init__(self, files, transform = None):
    self.files = files
    self.transform = transform

  def __len__(self):
    return len(self.files)

  def __getitem__(self,idx): # Need idx!
    data = self.files[idx]
    waveform = data['waveform']
    label = data['label']
    # I don't need sr and speaker
    if self.transform:
      waveform = self.transform(waveform)

    return waveform, label

# Make dataloader
traindataset = audiodataset(dataset)
print(traindataset[0]) # waveform, label
train_loader = DataLoader(traindataset, batch_size = 32, shuffle = True)
for batch in train_loader:
  waveform, label = batch
  print(waveform.shape)
  print(label.shape)
  break # watch just one example



# using GPU
if torch.cuda.is_available():
  device = torch.device('cuda')
  print('GPU is availble')
else:
  device = torch.device('cpu')
  print(f'YOU CANNOT NOT USE GPU!!!')

# Make simple LSTM model
# the big difference of LSTM and RNN is Long term memory (cell state)
import torch.nn as nn
class LSTM(nn.Module):
  def __init__ (self, input_dim, hidden_size, num_layers, num_classes):
    super(LSTM, self).__init__()
    self.hidden_size = hidden_size
    self.num_layers = num_layers
    self.lstm = nn.LSTM(input_dim, hidden_size, num_layers)
    self.fc = nn.Linear(hidden_size, num_classes)

  def forward(self, x):
    out, (hn,cn) = self.lstm(x) # last time hidden state, last time cell state
    out = self.fc(out[:,-1,:]) # need only last time step -> -1
    return out

model = LSTM(1,128, 2, 10)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(),lr = 0.0001)
print(model)

# training
epochs = 5

for epoch in range(1,epochs+1):
  loss_epoch = 0
  accuracy_epoch = 0

  for batch in train_loader:
    wave,label = batch
    # to into the LSTM, x should be [batch, seq_length, input_dim] but my waveform size is [batch, input_dim, seq_length] -> need to change
    wave = wave.transpose(1,2)
   # wave = wave.to(device)
   # label = label.to(device)
    optimizer.zero_grad()
    output = model(wave)
    loss = criterion(output,label)
    loss.backward()
    optimizer.step()
    loss_epoch += loss.item()
    y_pred = torch.argmax(output,dim=1)
    accuracy_epoch += (y_pred==label).sum().item()

  loss_avg = loss_epoch/len(train_loader)
  accuracy_avg = accuracy_epoch/len(traindataset)
  print(f'Epoch : {epoch}, Loss : {loss_avg:10.8f}, Accuracy : {accuracy_avg:10.8f}')

# Without transform, it has very low accuracy
# Using MFCC
import torchaudio.transforms as T
mfcc = T.MFCC(16000,40) # sample_rate, dimension of vector of each frame-> feature
traindataset = audiodataset(dataset,transform = mfcc)
print(traindataset[0]) # waveform, label
train_loader = DataLoader(traindataset, batch_size = 32, shuffle = True)
for batch in train_loader:
  waveform, label = batch
  print(waveform.shape)
  print(label.shape)
  break # watch just one example

# make another model
model1 = LSTM(40,256, 2, 10)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model1.parameters(),lr = 0.0001)
print(model1)

# to into the model, I need to change(batch, input_dim, seq_length)
# training
epochs = 5

for epoch in range(1,epochs+1):
  loss_epoch = 0
  accuracy_epoch = 0

  for batch in train_loader:
    wave,label = batch
    wave = wave.squeeze(1) # (batch, input_dim, seq_len)
    wave = wave.transpose(1,2) #(batch, seq_len. input_dim)
   # wave = wave.to(device)
   # label = label.to(device)
    optimizer.zero_grad()
    output = model1(wave)
    loss = criterion(output,label)
    loss.backward()
    optimizer.step()
    loss_epoch += loss.item()
    y_pred = torch.argmax(output,dim=1)
    accuracy_epoch += (y_pred==label).sum().item()

  loss_avg = loss_epoch/len(train_loader)
  accuracy_avg = accuracy_epoch/len(traindataset)
  print(f'Epoch : {epoch}, Loss : {loss_avg:10.8f}, Accuracy : {accuracy_avg:10.8f}')

