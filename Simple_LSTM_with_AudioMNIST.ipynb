{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Simple LSTM with audio_mnist"
      ],
      "metadata": {
        "id": "tnYdWCq-08eI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@article{audiomnist2023,\n",
        "#    title = {AudioMNIST: Exploring Explainable Artificial Intelligence for audio analysis on a simple benchmark},\n",
        "#    journal = {Journal of the Franklin Institute},\n",
        "#    year = {2023},\n",
        "#    issn = {0016-0032},\n",
        "#    doi = {https://doi.org/10.1016/j.jfranklin.2023.11.038},\n",
        "#    url = {https://www.sciencedirect.com/science/article/pii/S0016003223007536},\n",
        "#    author = {Sören Becker and Johanna Vielhaben and Marcel Ackermann and Klaus-Robert Müller and Sebastian Lapuschkin and Wojciech Samek},\n",
        "#    keywords = {Deep learning, Neural networks, Interpretability, Explainable artificial intelligence, Audio classification, Speech recognition},\n",
        "#}"
      ],
      "metadata": {
        "id": "T-tl3H_gJspX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use kaggle audio mnist\n",
        "# https://www.kaggle.com/datasets/sripaadsrinivasan/audio-mnist\n",
        "# we need to make dataset class"
      ],
      "metadata": {
        "id": "coDh9h3PQTSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQV-zjZ_RvCc",
        "outputId": "d382a56f-2dab-4f63-e86e-e5496bfcf188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check the data -> we need to make labels\n",
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "path = '/content/drive/MyDrive/data'\n",
        "dataset = []\n",
        "for i in range(1,61):\n",
        "  number = f'{i:02d}' # ex) 01\n",
        "  audiofolder = os.path.join(path, number)\n",
        "  for audio in os.listdir(audiofolder): # search all files in audiofolder\n",
        "    audiopath = os.path.join(audiofolder, audio) # access to real audio data\n",
        "    # In file name, there are label, speaker, repeat number\n",
        "    filename = os.path.basename(audiopath)\n",
        "    audioname = filename.replace('wav','')\n",
        "    label = filename.split('_')[0]\n",
        "    speaker = filename.split('_')[1]\n",
        "    repeat = filename.split('_')[2]\n",
        "    waveform, samplerate = torchaudio.load(audiopath)\n",
        "    label = torch.tensor(int(label))\n",
        "    dataset.append({'waveform':waveform, 'sr':samplerate,'speaker':speaker,'label':label})\n",
        "  if i % 10 == 0:\n",
        "    print(f'Label {i}/60 complete')\n",
        "\n"
      ],
      "metadata": {
        "id": "uKRhHrXWRwRH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca8b2120-915e-47fc-eb8a-024fa5050cef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label 10/60 complete\n",
            "Label 20/60 complete\n",
            "Label 30/60 complete\n",
            "Label 40/60 complete\n",
            "Label 50/60 complete\n",
            "Label 60/60 complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Watch data example\n",
        "print(dataset[0])\n",
        "print(len(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym_nGKMWbe4q",
        "outputId": "96722fef-596c-4ae0-8cc3-f586458d146c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'waveform': tensor([[-0.0003, -0.0003, -0.0003,  ..., -0.0004, -0.0003, -0.0003]]), 'sr': 48000, 'speaker': '01', 'label': tensor(0)}\n",
            "30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I need to watch all the length of waveform is same\n",
        "# To watch the length od tensor, use shape[1]. because tensor = 1*14073\n",
        "A=[]\n",
        "for i in range(len(dataset)):\n",
        "  w = dataset[i]['waveform']\n",
        "  A.append(w.shape[1])\n",
        "print(f' Min : {min(A)}, Max: {max(A)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKTDMPH7QwQh",
        "outputId": "a501cde4-a79f-44d4-deda-b113c7f59112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Min : 14073, Max: 47998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# So, I need to make same length -> just fit it as 20KHz. using nn.functional.pad\n",
        "import torch.nn.functional as F\n",
        "A=[]\n",
        "for i in range(len(dataset)):\n",
        "  w = dataset[i]['waveform']\n",
        "  if w.shape[1] < 20000:\n",
        "    w =F.pad(w,(0,20000-w.shape[1])) # pad(x,(number, length to pad))\n",
        "    dataset[i]['waveform'] = w[:,:20000]\n",
        "  else:\n",
        "    dataset[i]['waveform'] = w[:,:20000]\n",
        "for i in range(len(dataset)):\n",
        "  w = dataset[i]['waveform']\n",
        "  A.append(w.shape[1])\n",
        "print(f' Min : {min(A)}, Max: {max(A)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksmsCQ-MS0Va",
        "outputId": "a21deb7f-178f-49d8-9a62-c190db701e57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Min : 20000, Max: 20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make DATASET class to pytorch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# make init,len,getitem\n",
        "class audiodataset(Dataset):\n",
        "  def __init__(self, files, transform = None):\n",
        "    self.files = files\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.files)\n",
        "\n",
        "  def __getitem__(self,idx): # Need idx!\n",
        "    data = self.files[idx]\n",
        "    waveform = data['waveform']\n",
        "    label = data['label']\n",
        "    # I don't need sr and speaker\n",
        "    if self.transform:\n",
        "      waveform = self.transform(waveform)\n",
        "\n",
        "    return waveform, label\n",
        "\n"
      ],
      "metadata": {
        "id": "G1qRAn8WcaHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make dataloader\n",
        "traindataset = audiodataset(dataset)\n",
        "print(traindataset[0]) # waveform, label\n",
        "train_loader = DataLoader(traindataset, batch_size = 32, shuffle = True)\n",
        "for batch in train_loader:\n",
        "  waveform, label = batch\n",
        "  print(waveform.shape)\n",
        "  print(label.shape)\n",
        "  break # watch just one example"
      ],
      "metadata": {
        "id": "7alrD6M2fOyt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0bd3cae-ebfe-4c5d-91af-550203b20144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[-0.0003, -0.0003, -0.0003,  ...,  0.0085,  0.0085,  0.0087]]), tensor(0))\n",
            "torch.Size([32, 1, 20000])\n",
            "torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aKqQ7h9naUGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using GPU\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "  print('GPU is availble')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "  print(f'YOU CANNOT NOT USE GPU!!!')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_eMbEWDXvS8",
        "outputId": "4ab139a3-6db3-4fed-fd09-e4b59c08527e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is availble\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make simple LSTM model\n",
        "# the big difference of LSTM and RNN is Long term memory (cell state)\n",
        "import torch.nn as nn\n",
        "class LSTM(nn.Module):\n",
        "  def __init__ (self, input_dim, hidden_size, num_layers, num_classes):\n",
        "    super(LSTM, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.lstm = nn.LSTM(input_dim, hidden_size, num_layers)\n",
        "    self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out, (hn,cn) = self.lstm(x) # last time hidden state, last time cell state\n",
        "    out = self.fc(out[:,-1,:]) # need only last time step -> -1\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "rHJwfzCKZ6do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Without transform, it has very low accuracy and so much time spent\n",
        "# Using MFCC\n",
        "import torchaudio.transforms as T\n",
        "mfcc = T.MFCC(16000,40) # sample_rate, dimension of vector of each frame-> feature , actually I want to use raw data, but it has too much length\n",
        "traindataset = audiodataset(dataset,transform = mfcc)\n",
        "print(traindataset[0]) # waveform, label\n",
        "train_loader = DataLoader(traindataset, batch_size = 32, shuffle = True)\n",
        "for batch in train_loader:\n",
        "  waveform, label = batch\n",
        "  print(waveform.shape)\n",
        "  print(label.shape)\n",
        "  break # watch just one example"
      ],
      "metadata": {
        "id": "HiukO-GsZL3P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7017b1b-2de1-48aa-8c53-4f9f0552fb70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[[-782.5766, -723.1873, -759.4866,  ..., -611.1515, -597.2868,\n",
            "          -587.9297],\n",
            "         [  12.3939,  -27.6590,  -12.8135,  ...,  125.8783,  132.2648,\n",
            "           109.6659],\n",
            "         [  34.7157,   37.2915,   30.8310,  ...,   51.4044,   67.8076,\n",
            "            37.6513],\n",
            "         ...,\n",
            "         [  -1.5433,   -9.1804,   -4.3317,  ...,   -8.1542,   -8.0709,\n",
            "           -12.1726],\n",
            "         [ -10.6302,   -8.4885,    1.1315,  ...,  -23.6739,  -12.7377,\n",
            "           -11.2347],\n",
            "         [  -5.1951,   -4.0581,   -4.2072,  ...,   -8.1796,   -5.6248,\n",
            "           -11.8580]]]), tensor(0))\n",
            "torch.Size([32, 1, 40, 101])\n",
            "torch.Size([32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchaudio/functional/functional.py:585: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make another model\n",
        "model1 = LSTM(40,256, 2, 10)\n",
        "model1.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model1.parameters(),lr = 0.0001)\n",
        "print(model1)"
      ],
      "metadata": {
        "id": "RQfUoeImev8E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "434bc70d-0cdb-4277-e0c7-d08debc65f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM(\n",
            "  (lstm): LSTM(40, 256, num_layers=2)\n",
            "  (fc): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to into the model, I need to change(batch, input_dim, seq_length)\n",
        "# training\n",
        "epochs = 7\n",
        "\n",
        "for epoch in range(1,epochs+1):\n",
        "  loss_epoch = 0\n",
        "  accuracy_epoch = 0\n",
        "\n",
        "  for batch in train_loader:\n",
        "    wave,label = batch\n",
        "    wave = wave.squeeze(1) # (batch, input_dim, seq_len)\n",
        "    wave = wave.transpose(1,2) #(batch, seq_len. input_dim)\n",
        "    wave = wave.to(device)\n",
        "    label = label.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model1(wave)\n",
        "    loss = criterion(output,label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss_epoch += loss.item()\n",
        "    y_pred = torch.argmax(output,dim=1)\n",
        "    accuracy_epoch += (y_pred==label).sum().item()\n",
        "\n",
        "  loss_avg = loss_epoch/len(train_loader)\n",
        "  accuracy_avg = accuracy_epoch/len(traindataset)\n",
        "  print(f'Epoch : {epoch}, Loss : {loss_avg:10.8f}, Accuracy : {accuracy_avg:10.8f}')"
      ],
      "metadata": {
        "id": "f4PM96pOeHgf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6b9723a-078e-41d2-8b78-8c2addb0f842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 1, Loss : 1.97898130, Accuracy : 0.26900000\n",
            "Epoch : 2, Loss : 1.71708978, Accuracy : 0.36246667\n",
            "Epoch : 3, Loss : 1.64677118, Accuracy : 0.39673333\n",
            "Epoch : 4, Loss : 1.60159504, Accuracy : 0.41486667\n",
            "Epoch : 5, Loss : 1.56891212, Accuracy : 0.42770000\n",
            "Epoch : 6, Loss : 1.53923784, Accuracy : 0.44056667\n",
            "Epoch : 7, Loss : 1.51543413, Accuracy : 0.44550000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I want to compare with the accuracy of raw audio, but it spent too much resources. And, This model has only 0.44 Accuracy of train dataset. This project is just using  simple LSTM with AUdiomnist, so end here and later, I need to study more deep in the data preprocessing and LSTM structure"
      ],
      "metadata": {
        "id": "GDavk0DiVsdP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MbquLFJPFgSf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}