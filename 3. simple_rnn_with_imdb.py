# -*- coding: utf-8 -*-
"""Simple_RNN_with_IMDB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lrYJOKZRp1cbDs1om-GSIod6Ri7xvgn1
"""
# it has error when upload as ipynb.......
# Recurrent Neural Network - RNN
# it uses 'memory' : past thing influences result
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

# I will use imdb dataset
from datasets import load_dataset
dataset = load_dataset("imdb")
print(dataset)

# I just want to use train,test
train_dataset = dataset['train']
test_dataset = dataset['test']

# watch example of dataset - label:0 'negative, 1 'positive'
print(train_dataset[0])

# So I need to change text to number -> use tokenizer
# BERT tokenizer(subword)
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

# batch:dict(text,label), make all length of sequences same
def tokenize(batch):
  return tokenizer(batch['text'],padding=True, max_length = 256,truncation=True)

# using map to tokenize
train_dataset_token = train_dataset.map(tokenize, batched = True)
test_dataset_token = test_dataset.map(tokenize, batched = True)

# new keys are made
print(train_dataset_token[0].keys())
# input_ids: sequence that token to number 101 to start, 102 to finish, 0 is padding
# attention_mask : 1:real token, 0:padding token
# token_type_dis : show the division of text -> not use in this study

# Change the format to use torch (use only input_ids, 'attention_mask', label)
train_dataset_token.set_format('torch', columns=['input_ids','attention_mask','label'])
test_dataset_token.set_format('torch', columns=['input_ids','attention_mask','label'])

print(train_dataset_token[0])

# Making dataloader
from torch.utils.data import DataLoader
train_loader = DataLoader(train_dataset_token, batch_size = 64, shuffle = True)
test_loader = DataLoader(test_dataset_token, batch_size = 64, shuffle = True)

# Vocab: the set of whole tokens. It connects word and id
print(tokenizer.vocab_size)
print(list(tokenizer.vocab)[:10])

# Using GPU
if torch.cuda.is_available():
  device = torch.device('cuda')
  print('GPU is available')
else:
  print('GPU is not available')

# Make simple RNN model
class RNN(nn.Module):
  def __init__(self, vocab_size, embed_dim, hidden_size, num_layers, num_classes):
    super(RNN, self).__init__()
    self.embedding = nn.Embedding(vocab_size, embed_dim) # word embedding(28996, show the dimension of vector) -> shape(sequnce length, embedding_dimension)
    # RNN(the dimension of token vector, the size of hidden layer made by RNN, layer of RNN, True: (batch, sequence length, embedding dimension))
    self.rnn = nn.RNN(embed_dim, hidden_size, num_layers, batch_first = True)
    self.fc = nn.Linear(hidden_size, num_classes)

  def forward(self,x):
    x = self.embedding(x)
    x, h_n = self.rnn(x)
    h_n_last = h_n[-1] # In RNN, the results are accumulated in last layer
    out = self.fc(h_n_last)
    return out

model = RNN(vocab_size = tokenizer.vocab_size, embed_dim = 128, hidden_size = 256, num_layers = 2, num_classes = 2) # num_classes -> negative or positive)
model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)
print(model)

# Let's go 30 -> It takes so much time....
epochs = 30
for epoch in range(1,epochs+1):
  loss_epoch = 0
  accuracy_epoch = 0

  for batch in train_loader:
    optimizer.zero_grad() # Don't forget!!
    input_ids = batch['input_ids'].to(device)
    label = batch['label'].to(device)
    outputs = model(input_ids)
    loss = criterion(outputs, label)
    loss.backward()
    optimizer.step()

    loss_epoch += loss.item()
    y_pred = outputs.argmax(dim=1)
    accuracy_epoch += (y_pred == label).sum().item()

  loss_avg = loss_epoch/len(train_loader)
  accuracy_avg = accuracy_epoch/len(train_dataset)
  if epoch % 5 == 0:
    print(f'Epoch: {epoch}, Loss: {loss_avg:10.8f}, Accuracy: {accuracy_avg:10.8f}')

# Find test accuracy
accuracy = 0
loss = 0
for batch in test_loader:
  input_ids = batch['input_ids'].to(device)
  label = batch['label'].to(device)
  outputs = model(input_ids)
  y_pred = outputs.argmax(dim=1)
  loss = criterion(outputs, label)
  accuracy += (y_pred == label).sum().item()
  loss += loss.item()

accuracy_avg = accuracy/len(test_dataset)
loss_avg = loss/len(test_loader)
print(f'Test set accuracy: {accuracy_avg:10.8f}, loss: {loss_avg:10.8f}')

